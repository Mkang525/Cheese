{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOZZARELLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mozzarella.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "url = 'https://cheese.com/mozzarella/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "mozz= soup.find('div', class_='unit')\n",
    "mozzarella=mozz.h1.text\n",
    "mozzarella\n",
    "\n",
    "with open(\"mozzarella.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_mozz=first1[0]\n",
    "        attribute_mozz=first1[1]\n",
    "    \n",
    "        writer.writerow([category_mozz, attribute_mozz, mozzarella])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BURRATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"burrata.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://cheese.com/burrata/'\n",
    "browser.visit(url1)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url1)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>Made from pasteurized or unpasteurized <a href=\"/by_milk/?m=water-buffalo\">water buffalo</a>'s milk</p>,\n",
       " <p>Country of origin: <a href=\"/by_country/?c=IT\">Italy</a> and <a href=\"/by_country/?c=US\">United States</a></p>,\n",
       " <p>Region: Apulia </p>,\n",
       " <p>Family: Mozzarella</p>,\n",
       " <p>Type: <a href=\"/by_type/?t=fresh-soft\">fresh soft</a>, artisan</p>,\n",
       " <p>Texture: <a href=\"/by_texture/?t=creamy\">creamy</a> and <a href=\"/by_texture/?t=stringy\">stringy</a></p>,\n",
       " <p>Rind: leaf wrapped</p>,\n",
       " <p>Colour: white</p>,\n",
       " <p>Flavour: buttery, milky</p>,\n",
       " <p>Aroma: fresh, milky</p>,\n",
       " <p>Vegetarian:  no </p>,\n",
       " <p>Producers: BelGioioso Cheese Inc., Fiore di Nonno</p>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Burrata '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burr= soup.find('div', class_='unit')\n",
    "burrata=burr.h1.text\n",
    "burrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"burrata.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_burr=first1[0]\n",
    "        attribute_burr=first1[1]\n",
    "        \n",
    "        writer.writerow([burrata, category_burr, attribute_burr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEVRE (GOAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chevre.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://cheese.com/chevre/'\n",
    "browser.visit(url2)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url2)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>Made from pasteurized or unpasteurized <a href=\"/by_milk/?m=goat\">goat</a>'s milk</p>,\n",
       " <p>Country of origin: <a href=\"/by_country/?c=FR\">France</a></p>,\n",
       " <p>Type: <a href=\"/by_type/?t=soft\">soft</a>, <a href=\"/by_type/?t=semi-soft\">semi-soft</a>, <a href=\"/by_type/?t=hard\">hard</a>, <a href=\"/by_type/?t=firm\">firm</a></p>,\n",
       " <p>Texture: <a href=\"/by_texture/?t=firm\">firm</a> and <a href=\"/by_texture/?t=soft\">soft</a></p>,\n",
       " <p>Colour: white</p>,\n",
       " <p>Flavour: earthy, tangy, tart</p>,\n",
       " <p>Synonyms: Goat cheese</p>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chevre '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chev= soup.find('div', class_='unit')\n",
    "chevre=chev.h1.text\n",
    "chevre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chevre.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_chev=first1[0]\n",
    "        attribute_chev=first1[1]\n",
    "        \n",
    "        writer.writerow([chevre, category_chev, attribute_chev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feta.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url3 = 'https://cheese.com/feta/'\n",
    "browser.visit(url3)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url3)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "fet= soup.find('div', class_='unit')\n",
    "feta=fet.h1.text\n",
    "\n",
    "with open(\"feta.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_feta=first1[0]\n",
    "        attribute_feta=first1[1]\n",
    "        \n",
    "        writer.writerow([feta, category_feta, attribute_feta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RICOTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ricotta.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url4 = 'https://cheese.com/fresh-ricotta/'\n",
    "browser.visit(url4)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url4)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "ric= soup.find('div', class_='unit')\n",
    "ricotta=ric.h1.text\n",
    "\n",
    "with open(\"ricotta.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_ric=first1[0]\n",
    "        attribute_ric=first1[1]\n",
    "        \n",
    "        writer.writerow([ricotta, category_ric, attribute_ric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARSCAPONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"marscapone.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url5 = 'https://cheese.com/marscapone/'\n",
    "browser.visit(url5)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url5)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "mars= soup.find('div', class_='unit')\n",
    "marscapone=mars.h1.text\n",
    "\n",
    "with open(\"marscapone.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_mars=first1[0]\n",
    "        attribute_mars=first1[1]\n",
    "        \n",
    "        writer.writerow([marscapone, category_mars, attribute_mars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRESCENZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crescenza.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url6 = 'https://cheese.com/crescenza/'\n",
    "browser.visit(url6)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url6)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "cres= soup.find('div', class_='unit')\n",
    "crescenza=cres.h1.text\n",
    "\n",
    "with open(\"crescenza.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_cres=first1[0]\n",
    "        attribute_cres=first1[1]\n",
    "        \n",
    "        writer.writerow([crescenza, category_cres, attribute_cres])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOURSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"boursin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url7 = 'https://cheese.com/boursin/'\n",
    "browser.visit(url7)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url7)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "bou= soup.find('div', class_='unit')\n",
    "boursin=bou.h1.text\n",
    "\n",
    "with open(\"boursin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_bou=first1[0]\n",
    "        attribute_bou=first1[1]\n",
    "        \n",
    "        writer.writerow([boursin, category_bou, attribute_bou])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELLES SUR CHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sellessurcher.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url8 = 'https://cheese.com/selles-sur-cher/'\n",
    "browser.visit(url8)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url8)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "sell= soup.find('div', class_='unit')\n",
    "sellessurcher=sell.h1.text\n",
    "\n",
    "with open(\"sellessurcher.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_sel=first1[0]\n",
    "        attribute_sel=first1[1]\n",
    "        \n",
    "        writer.writerow([sellessurcher, category_sel, attribute_sel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"brie.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url9 = 'https://cheese.com/brie/'\n",
    "browser.visit(url9)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url9)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "bri= soup.find('div', class_='unit')\n",
    "brie=bri.h1.text\n",
    "\n",
    "with open(\"brie.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_bri=first1[0]\n",
    "        attribute_bri=first1[1]\n",
    "        \n",
    "        writer.writerow([brie, category_bri, attribute_bri])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAMEMBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"camembert.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url10 = 'https://cheese.com/camembert/'\n",
    "browser.visit(url10)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url10)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "cam=soup.find('div', class_='unit')\n",
    "camembert=cam.h1.text\n",
    "\n",
    "with open(\"camembert.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_cam=first1[0]\n",
    "        attribute_cam=first1[1]\n",
    "        \n",
    "        writer.writerow([camembert, category_cam, attribute_cam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chaource.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url11 = 'https://cheese.com/chaource/'\n",
    "browser.visit(url11)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url11)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "cha=soup.find('div', class_='unit')\n",
    "chaource=cha.h1.text\n",
    "\n",
    "with open(\"chaource.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_cha=first1[0]\n",
    "        attribute_cha=first1[1]\n",
    "        \n",
    "        writer.writerow([chaource, category_cha, attribute_cha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neufchâtel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"neufchatel.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url12 = 'https://cheese.com/neufchatel/'\n",
    "browser.visit(url12)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url12)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "neu=soup.find('div', class_='unit')\n",
    "neufchatel=neu.h1.text\n",
    "\n",
    "with open(\"neuchatel.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_neu=first1[0]\n",
    "        attribute_neu=first1[1]\n",
    "        \n",
    "        writer.writerow([neufchatel, category_neu, attribute_neu])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROTTIN DE CHAVIGNOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crottin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url13 = 'https://cheese.com/crottin-de-chavignol/'\n",
    "browser.visit(url13)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url13)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "cro=soup.find('div', class_='unit')\n",
    "crottin=cro.h1.text\n",
    "\n",
    "with open(\"crottin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_cro=first1[0]\n",
    "        attribute_cro=first1[1]\n",
    "        \n",
    "        writer.writerow([crottin, category_cro, attribute_cro])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FONTINA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fontina.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url14 = 'https://cheese.com/fontina/'\n",
    "browser.visit(url14)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url14)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "fon=soup.find('div', class_='unit')\n",
    "fontina=fon.h1.text\n",
    "\n",
    "with open(\"fontina.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([fontina, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPOISSES DE BOURGOGNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"epoisses.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url15 ='https://cheese.com/epoisses-de-bourgogne/'\n",
    "browser.visit(url15)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url15)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "epo=soup.find('div', class_='unit')\n",
    "epoisses=fon.h1.text\n",
    "\n",
    "with open(\"epoisses.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([epoisses, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REBLOCHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reblochon.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url16 ='https://cheese.com/reblochon/'\n",
    "browser.visit(url16)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url16)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "reb=soup.find('div', class_='unit')\n",
    "reblochon=reb.h1.text\n",
    "\n",
    "with open(\"reblochon.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([reblochon, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALEGGIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"taleggio.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url17 ='https://cheese.com/taleggio/'\n",
    "browser.visit(url17)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url17)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "tal=soup.find('div', class_='unit')\n",
    "taleggio=tal.h1.text\n",
    "\n",
    "with open(\"taleggio.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([taleggio, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"langres.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url18 ='https://cheese.com/langres/'\n",
    "browser.visit(url18)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url18)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "lan=soup.find('div', class_='unit')\n",
    "langres=tal.h1.text\n",
    "\n",
    "with open(\"langres.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([langres, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAUMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chaumes.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url19 ='https://cheese.com/chaumes/'\n",
    "browser.visit(url19)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url19)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "chau=soup.find('div', class_='unit')\n",
    "chaumes=chau.h1.text\n",
    "\n",
    "with open(\"chaumes.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([chaumes, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIVAROT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"livarot.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url20 ='https://cheese.com/livarot/'\n",
    "browser.visit(url20)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url20)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "liv=soup.find('div', class_='unit')\n",
    "livarot=liv.h1.text\n",
    "\n",
    "with open(\"livarot.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([livarot, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUNSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"munster.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url21 ='https://cheese.com/munster/'\n",
    "browser.visit(url21)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url21)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "mun=soup.find('div', class_='unit')\n",
    "munster=mun.h1.text\n",
    "\n",
    "with open(\"munster.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([munster, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VACHERIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vacherin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url22 ='https://cheese.com/vacherin/'\n",
    "browser.visit(url22)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url22)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "vach=soup.find('div', class_='unit')\n",
    "vacherin=vach.h1.text\n",
    "\n",
    "with open(\"vacherin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([vacherin, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRUYERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gruyere.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url23='https://cheese.com/gruyere/'\n",
    "browser.visit(url23)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url23)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "gru=soup.find('div', class_='unit')\n",
    "gruyere=gru.h1.text\n",
    "\n",
    "with open(\"gruyere.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([gruyere, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gouda.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url24='https://cheese.com/gouda/'\n",
    "browser.visit(url24)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url24)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "gou=soup.find('div', class_='unit')\n",
    "gouda=gou.h1.text\n",
    "\n",
    "with open(\"gouda.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([gouda, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAVARTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"havarti.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url25='https://cheese.com/cream-havarti/'\n",
    "browser.visit(url25)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url25)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "hav=soup.find('div', class_='unit')\n",
    "havarti=hav.h1.text\n",
    "\n",
    "with open(\"havarti.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([havarti, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROVOLONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"provolone.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url26='https://cheese.com/provolone/'\n",
    "browser.visit(url26)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url26)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "prov=soup.find('div', class_='unit')\n",
    "provolone=prov.h1.text\n",
    "\n",
    "with open(\"provolone.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([provolone, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edam.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url27='https://cheese.com/edam/'\n",
    "browser.visit(url27)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url27)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "ed=soup.find('div', class_='unit')\n",
    "edam=ed.h1.text\n",
    "\n",
    "with open(\"edam.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([edam, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORBIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"morbier.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url28='https://cheese.com/morbier/'\n",
    "browser.visit(url28)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url28)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "mor=soup.find('div', class_='unit')\n",
    "morbier=mor.h1.text\n",
    "\n",
    "with open(\"morbier.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([morbier, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMOLETTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mimolette.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url29='https://cheese.com/mimolette-boule-de-lille/'\n",
    "browser.visit(url29)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url29)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "mim=soup.find('div', class_='unit')\n",
    "mimolette=mim.h1.text\n",
    "\n",
    "with open(\"mimolette.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([mimolette, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEDDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cheddar.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url30='https://cheese.com/cheddar/'\n",
    "browser.visit(url30)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url30)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "ched=soup.find('div', class_='unit')\n",
    "cheddar=ched.h1.text\n",
    "\n",
    "with open(\"cheddar.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([cheddar, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOUBLE GLOUCESTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"doublegloucester.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url31='https://cheese.com/double-gloucester/'\n",
    "browser.visit(url31)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url31)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "glou=soup.find('div', class_='unit')\n",
    "doublegloucester=glou.h1.text\n",
    "\n",
    "with open(\"doublegloucester.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([doublegloucester, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARMESAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parmesan.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url32='https://cheese.com/parmesan/'\n",
    "browser.visit(url32)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url32)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "parm=soup.find('div', class_='unit')\n",
    "parmesan=parm.h1.text\n",
    "\n",
    "with open(\"parmesan.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([parmesan, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PECORINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pecorino.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url33='https://cheese.com/pecorino/'\n",
    "browser.visit(url33)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url33)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "pec=soup.find('div', class_='unit')\n",
    "pecorino=pec.h1.text\n",
    "\n",
    "with open(\"pecorino.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([pecorino, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MANCHEGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"manchego.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url34='https://cheese.com/manchego/'\n",
    "browser.visit(url34)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url34)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "man=soup.find('div', class_='unit')\n",
    "manchego=man.h1.text\n",
    "\n",
    "with open(\"manchego.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([manchego, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRANA PADANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"granapadano.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url35='https://cheese.com/grana-padano/'\n",
    "browser.visit(url35)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url35)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "gra=soup.find('div', class_='unit')\n",
    "granapadano=gra.h1.text\n",
    "\n",
    "with open(\"granapadano.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([granapadano, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEAUFORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"beaufort.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url36='https://cheese.com/beaufort/'\n",
    "browser.visit(url36)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url36)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "beau=soup.find('div', class_='unit')\n",
    "beaufort=beau.h1.text\n",
    "\n",
    "with open(\"beaufort.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([beaufort, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CANTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cantal.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url37='https://cheese.com/cantal/'\n",
    "browser.visit(url37)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url37)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "can=soup.find('div', class_='unit')\n",
    "cantal=can.h1.text\n",
    "\n",
    "with open(\"cantal.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([cantal, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMMENTHAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"emmental.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url38='https://cheese.com/emmental/'\n",
    "browser.visit(url38)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url38)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "emm=soup.find('div', class_='unit')\n",
    "emmental=emm.h1.text\n",
    "\n",
    "with open(\"emmental.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([emmental, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBRINZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sbrinz.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url39='https://cheese.com/sbrinz/'\n",
    "browser.visit(url39)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url39)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "sbr=soup.find('div', class_='unit')\n",
    "sbrinz=sbr.h1.text\n",
    "\n",
    "with open(\"sbrinz.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([sbrinz, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
