{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOZZARELLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mozzarella.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "url = 'https://cheese.com/mozzarella/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "mozz= soup.find('div', class_='unit')\n",
    "mozzarella=mozz.h1.text\n",
    "mozzarella\n",
    "\n",
    "with open(\"mozzarella.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_mozz=first1[0]\n",
    "        attribute_mozz=first1[1]\n",
    "    \n",
    "        writer.writerow([category_mozz, attribute_mozz, mozzarella])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BURRATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"burrata.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://cheese.com/burrata/'\n",
    "browser.visit(url1)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url1)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>Made from pasteurized or unpasteurized <a href=\"/by_milk/?m=water-buffalo\">water buffalo</a>'s milk</p>,\n",
       " <p>Country of origin: <a href=\"/by_country/?c=IT\">Italy</a> and <a href=\"/by_country/?c=US\">United States</a></p>,\n",
       " <p>Region: Apulia </p>,\n",
       " <p>Family: Mozzarella</p>,\n",
       " <p>Type: <a href=\"/by_type/?t=fresh-soft\">fresh soft</a>, artisan</p>,\n",
       " <p>Texture: <a href=\"/by_texture/?t=creamy\">creamy</a> and <a href=\"/by_texture/?t=stringy\">stringy</a></p>,\n",
       " <p>Rind: leaf wrapped</p>,\n",
       " <p>Colour: white</p>,\n",
       " <p>Flavour: buttery, milky</p>,\n",
       " <p>Aroma: fresh, milky</p>,\n",
       " <p>Vegetarian:  no </p>,\n",
       " <p>Producers: BelGioioso Cheese Inc., Fiore di Nonno</p>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Burrata '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burr= soup.find('div', class_='unit')\n",
    "burrata=burr.h1.text\n",
    "burrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"burrata.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_burr=first1[0]\n",
    "        attribute_burr=first1[1]\n",
    "        \n",
    "        writer.writerow([burrata, category_burr, attribute_burr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEVRE (GOAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chevre.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://cheese.com/chevre/'\n",
    "browser.visit(url2)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url2)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>Made from pasteurized or unpasteurized <a href=\"/by_milk/?m=goat\">goat</a>'s milk</p>,\n",
       " <p>Country of origin: <a href=\"/by_country/?c=FR\">France</a></p>,\n",
       " <p>Type: <a href=\"/by_type/?t=soft\">soft</a>, <a href=\"/by_type/?t=semi-soft\">semi-soft</a>, <a href=\"/by_type/?t=hard\">hard</a>, <a href=\"/by_type/?t=firm\">firm</a></p>,\n",
       " <p>Texture: <a href=\"/by_texture/?t=firm\">firm</a> and <a href=\"/by_texture/?t=soft\">soft</a></p>,\n",
       " <p>Colour: white</p>,\n",
       " <p>Flavour: earthy, tangy, tart</p>,\n",
       " <p>Synonyms: Goat cheese</p>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chevre '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chev= soup.find('div', class_='unit')\n",
    "chevre=chev.h1.text\n",
    "chevre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chevre.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_chev=first1[0]\n",
    "        attribute_chev=first1[1]\n",
    "        \n",
    "        writer.writerow([chevre, category_chev, attribute_chev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feta.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url3 = 'https://cheese.com/feta/'\n",
    "browser.visit(url3)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url3)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "fet= soup.find('div', class_='unit')\n",
    "feta=fet.h1.text\n",
    "\n",
    "with open(\"feta.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_feta=first1[0]\n",
    "        attribute_feta=first1[1]\n",
    "        \n",
    "        writer.writerow([feta, category_feta, attribute_feta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RICOTTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ricotta.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url4 = 'https://cheese.com/fresh-ricotta/'\n",
    "browser.visit(url4)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url4)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "ric= soup.find('div', class_='unit')\n",
    "ricotta=ric.h1.text\n",
    "\n",
    "with open(\"ricotta.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_ric=first1[0]\n",
    "        attribute_ric=first1[1]\n",
    "        \n",
    "        writer.writerow([ricotta, category_ric, attribute_ric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARSCAPONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"marscapone.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url5 = 'https://cheese.com/marscapone/'\n",
    "browser.visit(url5)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url5)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "mars= soup.find('div', class_='unit')\n",
    "marscapone=mars.h1.text\n",
    "\n",
    "with open(\"marscapone.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_mars=first1[0]\n",
    "        attribute_mars=first1[1]\n",
    "        \n",
    "        writer.writerow([marscapone, category_mars, attribute_mars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRESCENZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crescenza.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url6 = 'https://cheese.com/crescenza/'\n",
    "browser.visit(url6)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url6)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "cres= soup.find('div', class_='unit')\n",
    "crescenza=cres.h1.text\n",
    "\n",
    "with open(\"crescenza.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_cres=first1[0]\n",
    "        attribute_cres=first1[1]\n",
    "        \n",
    "        writer.writerow([crescenza, category_cres, attribute_cres])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOURSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"boursin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url7 = 'https://cheese.com/boursin/'\n",
    "browser.visit(url7)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url7)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "bou= soup.find('div', class_='unit')\n",
    "boursin=bou.h1.text\n",
    "\n",
    "with open(\"boursin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_bou=first1[0]\n",
    "        attribute_bou=first1[1]\n",
    "        \n",
    "        writer.writerow([boursin, category_bou, attribute_bou])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELLES SUR CHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sellessurcher.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url8 = 'https://cheese.com/selles-sur-cher/'\n",
    "browser.visit(url8)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url8)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "sell= soup.find('div', class_='unit')\n",
    "sellessurcher=sell.h1.text\n",
    "\n",
    "with open(\"sellessurcher.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_sel=first1[0]\n",
    "        attribute_sel=first1[1]\n",
    "        \n",
    "        writer.writerow([sellessurcher, category_sel, attribute_sel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"brie.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url9 = 'https://cheese.com/brie/'\n",
    "browser.visit(url9)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url9)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "bri= soup.find('div', class_='unit')\n",
    "brie=bri.h1.text\n",
    "\n",
    "with open(\"brie.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_bri=first1[0]\n",
    "        attribute_bri=first1[1]\n",
    "        \n",
    "        writer.writerow([brie, category_bri, attribute_bri])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAMEMBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"camembert.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url10 = 'https://cheese.com/camembert/'\n",
    "browser.visit(url10)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url10)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "cam=soup.find('div', class_='unit')\n",
    "camembert=cam.h1.text\n",
    "\n",
    "with open(\"camembert.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_cam=first1[0]\n",
    "        attribute_cam=first1[1]\n",
    "        \n",
    "        writer.writerow([camembert, category_cam, attribute_cam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chaource.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url11 = 'https://cheese.com/chaource/'\n",
    "browser.visit(url11)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url11)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "cha=soup.find('div', class_='unit')\n",
    "chaource=cha.h1.text\n",
    "\n",
    "with open(\"chaource.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_cha=first1[0]\n",
    "        attribute_cha=first1[1]\n",
    "        \n",
    "        writer.writerow([chaource, category_cha, attribute_cha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neufchâtel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"neufchatel.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url12 = 'https://cheese.com/neufchatel/'\n",
    "browser.visit(url12)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url12)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "neu=soup.find('div', class_='unit')\n",
    "neufchatel=neu.h1.text\n",
    "\n",
    "with open(\"neuchatel.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_neu=first1[0]\n",
    "        attribute_neu=first1[1]\n",
    "        \n",
    "        writer.writerow([neufchatel, category_neu, attribute_neu])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROTTIN DE CHAVIGNOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crottin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url13 = 'https://cheese.com/crottin-de-chavignol/'\n",
    "browser.visit(url13)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url13)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "cro=soup.find('div', class_='unit')\n",
    "crottin=cro.h1.text\n",
    "\n",
    "with open(\"crottin.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category_cro=first1[0]\n",
    "        attribute_cro=first1[1]\n",
    "        \n",
    "        writer.writerow([crottin, category_cro, attribute_cro])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FONTINA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fontina.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url14 = 'https://cheese.com/fontina/'\n",
    "browser.visit(url14)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url14)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "fon=soup.find('div', class_='unit')\n",
    "fontina=fon.h1.text\n",
    "\n",
    "with open(\"fontina.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([fontina, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPOISSES DE BOURGOGNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"epoisses.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url15 ='https://cheese.com/epoisses-de-bourgogne/'\n",
    "browser.visit(url15)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url15)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "epo=soup.find('div', class_='unit')\n",
    "epoisses=fon.h1.text\n",
    "\n",
    "with open(\"epoisses.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([epoisses, category, attribute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REBLOCHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reblochon.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "url16 ='https://cheese.com/reblochon/'\n",
    "browser.visit(url16)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url16)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "result = soup.find(\"ul\", class_ = \"summary-points\").find_all('p')\n",
    "\n",
    "reb=soup.find('div', class_='unit')\n",
    "reblochon=reb.h1.text\n",
    "\n",
    "with open(\"reblochon.csv\", \"a\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for p in result[1:]:\n",
    "        first=p.text\n",
    "        first1=first.split(\":\")\n",
    "        category=first1[0]\n",
    "        attribute=first1[1]\n",
    "        \n",
    "        writer.writerow([reblochon, category, attribute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
